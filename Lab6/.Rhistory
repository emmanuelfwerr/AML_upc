# Approximating the 1st function, "St. Johns"
#pdf(file="Canadian_FPCA_aprox_1st_function.pdf",width=8,height=6)
op<-par(mfrow=c(2,2))
plot(tempfd[1],ylim=c(-20,20), main="Only functional mean")
lines(mtempfd,col=2)
plot(tempfd[1],ylim=c(-20,20), main="mean + 1 FPC")
lines(ex.temp.r1,col=3)
plot(tempfd[1],ylim=c(-20,20), main="mean + 2 FPC")
lines(ex.temp.r2,col=4)
plot(tempfd[1],ylim=c(-20,20), main="mean + 3 FPC")
lines(ex.temp.r3,col=6)
par(op)
## map of scores
scores123 <- as.data.frame(scores[,1:3])
names(scores123) <-paste("FPC",1:6," (",round(100*tempPCA$varprop),"%)",sep="")[1:3]
plot(scores123)
#pdf(file="Canadian_FPCA_dim_reduct.pdf",width=8,height=6)
plot(scores123[,1:2])#,xlab="FPC 1",ylab="FPC 2")
text(scores123[,1],scores123[,2],CanadianWeather$place,pos=2+2*(scores123[,1]<0))
## Relation between FPCA scores and coordinates
plot(data.frame(CanadianWeather$coordinates,scores))
scores = tempPCA$scores
PCs = tempPCA$harmonics
# Firstly, just the mean + PC1
mtempfd = mean.fd(tempfd)
ex.temp.r1 = mtempfd + scores[1,1]*PCs[1]
# and plot these
plot(tempfd[1],ylim=c(-20,20))
lines(mtempfd,col=2)
lines(ex.temp.r1,col=3)
# Try adding the second PC
ex.temp.r2 = mtempfd + scores[1,1]*PCs[1] + scores[1,2]*PCs[2]
lines(ex.temp.r2,col=4)
# And the third
ex.temp.r3 = ex.temp.r2  + scores[1,3]*PCs[3]
lines(ex.temp.r3,col=6)
# Approximating the 1st function, "St. Johns"
#pdf(file="Canadian_FPCA_aprox_1st_function.pdf",width=8,height=6)
op<-par(mfrow=c(2,2))
plot(tempfd[1],ylim=c(-20,20), main="Only functional mean")
lines(mtempfd,col=2)
plot(tempfd[1],ylim=c(-20,20), main="mean + 1 FPC")
lines(ex.temp.r1,col=3)
plot(tempfd[1],ylim=c(-20,20), main="mean + 2 FPC")
lines(ex.temp.r2,col=4)
plot(tempfd[1],ylim=c(-20,20), main="mean + 3 FPC")
lines(ex.temp.r3,col=6)
par(op)
## map of scores
scores123 <- as.data.frame(scores[,1:3])
names(scores123) <-paste("FPC",1:6," (",round(100*tempPCA$varprop),"%)",sep="")[1:3]
plot(scores123)
#pdf(file="Canadian_FPCA_dim_reduct.pdf",width=8,height=6)
plot(scores123[,1:2])#,xlab="FPC 1",ylab="FPC 2")
text(scores123[,1],scores123[,2],CanadianWeather$place,pos=2+2*(scores123[,1]<0))
## Relation between FPCA scores and coordinates
plot(data.frame(CanadianWeather$coordinates,scores))
library(maps)
N.latitude <- CanadianWeather$coordinates[,1]
W.longitude <- CanadianWeather$coordinates[,2]
windows()
map('world',regions="Canada")
points(-W.longitude, N.latitude,pch=19,col=4)
text(-W.longitude, N.latitude,CanadianWeather$place,pos=3,col=4)
dev.set()
plot(-scores[,3],-scores[,1],
xlab=paste("-FPC3 (",round(100*tempPCA$varprop[3]),"%)",sep=""),
ylab=paste("-FPC1 (",round(100*tempPCA$varprop[1]),"%)",sep=""))
text(-scores[,3],-scores[,1],CanadianWeather$place,pos=2+2*(scores[,3]>0))
#pdf("Canada_and_FPCA.pdf",width=14,height=7)
op <- par(mfrow=c(1,2))
map('world',regions="Canada")
points(-W.longitude, N.latitude,pch=19,col=4)
text(-W.longitude, N.latitude,CanadianWeather$place,pos=3,col=4)
plot(-scores[,3],-scores[,1],
xlab=paste("-FPC3 (",round(100*tempPCA$varprop[3]),"%)",sep=""),
ylab=paste("-FPC1 (",round(100*tempPCA$varprop[1]),"%)",sep=""))
text(-scores[,3],-scores[,1],CanadianWeather$place,pos=2+2*(scores[,3]>0))
par(op)
## FPCA in fda.usc
library(fda.usc)
?fdata2pc
fdataobj<-fdata(mdata=t(CanadianWeather$dailyAv[,,1]),
argvals = day.5,
names = list(main="Canadian weather, Temperature",
xlab="Day",ylab="Temp Deg. C"))
plot(fdataobj)
tempPCA.fdata <- fdata2pc(fdataobj, ncomp=3)
summary(tempPCA.fdata)
dev.off()
names(tempPCA.fdata)
# % of explained variance by each principal component
round(tempPCA.fdata$d^2/sum(tempPCA.fdata$d^2)*100,2)
# cumulated
round(cumsum(tempPCA.fdata$d^2)/sum(tempPCA.fdata$d^2)*100,2)
plot(tempPCA.fdata$d^2/sum(tempPCA.fdata$d^2)*100,type="b",
main="Percentage of variance explained")
plot(tempPCA.fdata$rotation[1])
abline(h=0,col=8)
cte <-100
plot(fdataobj,col=8,main="Effect of the FPC 1")
lines(func.mean(fdataobj),lwd=4)
lines(func.mean(fdataobj)-cte*tempPCA.fdata$rotation[1],lwd=4,col=4,lty=2)
lines(func.mean(fdataobj)+cte*tempPCA.fdata$rotation[1],lwd=4,col=2,lty=2)
legend("topleft",c("mean - cte * FPC 1","mean","mean + cte * FPC 1"),
col=c(4,1,2),lty=c(2,1,2),lwd=4)
fdataobj<-fdata(mdata=t(CanadianWeather$dailyAv[,,1]),
argvals = day.5,
names = list(main="Canadian weather, Temperature",
xlab="Day",ylab="Temp Deg. C"))
plot(fdataobj)
tempPCA.fdata <- fdata2pc(fdataobj, ncomp=3)
summary(tempPCA.fdata)
dev.off()
summary(tempPCA.fdata)
dev.off()
names(tempPCA.fdata)
# % of explained variance by each principal component
round(tempPCA.fdata$d^2/sum(tempPCA.fdata$d^2)*100,2)
# cumulated
round(cumsum(tempPCA.fdata$d^2)/sum(tempPCA.fdata$d^2)*100,2)
plot(tempPCA.fdata$d^2/sum(tempPCA.fdata$d^2)*100,type="b",
main="Percentage of variance explained")
plot(tempPCA.fdata$rotation[1])
abline(h=0,col=8)
cte <-100
plot(fdataobj,col=8,main="Effect of the FPC 1")
lines(func.mean(fdataobj),lwd=4)
lines(func.mean(fdataobj)-cte*tempPCA.fdata$rotation[1],lwd=4,col=4,lty=2)
lines(func.mean(fdataobj)+cte*tempPCA.fdata$rotation[1],lwd=4,col=2,lty=2)
legend("topleft",c("mean - cte * FPC 1","mean","mean + cte * FPC 1"),
col=c(4,1,2),lty=c(2,1,2),lwd=4)
cte <-100
plot(fdataobj,col=8,main="Effect of the FPC 2")
lines(func.mean(fdataobj),lwd=4)
lines(func.mean(fdataobj)-cte*tempPCA.fdata$rotation[2],lwd=4,col=4,lty=2)
lines(func.mean(fdataobj)+cte*tempPCA.fdata$rotation[2],lwd=4,col=2,lty=2)
legend("topleft",c("mean - cte * FPC 2","mean","mean + cte * FPC 2"),
col=c(4,1,2),lty=c(2,1,2),lwd=4)
cte <-100
plot(fdataobj,col=8,main="Effect of the FPC 3")
lines(func.mean(fdataobj),lwd=4)
lines(func.mean(fdataobj)-cte*tempPCA.fdata$rotation[3],lwd=4,col=4,lty=2)
lines(func.mean(fdataobj)+cte*tempPCA.fdata$rotation[3],lwd=4,col=2,lty=2)
legend("topleft",c("mean - cte * FPC 3","mean","mean + cte * FPC 3"),
col=c(4,1,2),lty=c(2,1,2),lwd=4)
daybasis65 <- create.fourier.basis(c(0, 365), nbasis=65, period=365)
harmaccelLfd <- vec2Lfd(c(0,(2*pi/365)^2,0), c(0, 365))
harmfdPar     <- fdPar(daybasis65, harmaccelLfd, lambda=1e5)
daytempfd <- smooth.basis(day.5, CanadianWeather$dailyAv[,,"Temperature.C"],
daybasis65, fdnames=list("Day", "Station", "Deg C"))$fd
daytemppcaobj <- pca.fd(daytempfd, nharm=2, harmfdPar)
op <- par(mfrow=c(2,2))
plot(daytempfd,col=8)
lines(mean.fd(daytempfd),col=1,lwd=4)
plot(daytemppcaobj$harmonics,col=c(4,2),lwd=3)
plot.pca.fd(daytemppcaobj, cex.main=0.9)
par(op)
## Multidimensional scaling
## using library fda.usc
##
## by Pedro Delicado. May 2017
library(fda.usc)
fdataobj<-fdata(mdata=t(CanadianWeather$dailyAv[,,1]),
argvals = day.5,
names = list(main="Canadian weather, Temperature",
xlab="Day",ylab="Temp Deg. C"))
plot(fdataobj)
# distances and semimetrics in fda.usc
? metric.lp
? metric.kl
? semimetric.basis
? semimetric.NPFDA
d.temp.m <- metric.lp(fdataobj)
d.temp.sm <- semimetric.basis(fdataobj)
?cmdscale
mds.m <- cmdscale(d.temp.m,k=30,eig=TRUE)
mds.scores = mds.m$points[,1:3]
plot(as.data.frame(mds.scores))
# % of explained variance by each principal component
round(mds.m$eig/sum(mds.m$eig)*100,2)
# cumulated
round(cumsum(mds.m$eig)/sum(mds.m$eig)*100,2)
plot(mds.m$eig/sum(mds.m$eig)*100,type="b",
main="Percentage of variance explained")
# MDS scores maps
plot(mds.scores[,1],mds.scores[,2],xlab="PrCo 1",ylab="PrCo 2")
text(mds.scores[,1],mds.scores[,2],CanadianWeather$place,pos=3)
plot(-mds.scores[,3],mds.scores[,1],xlab="-PrCo 3",ylab="PrCo 1")
text(-mds.scores[,3],mds.scores[,1],CanadianWeather$place,pos=3)
## Relation between FPCA scores and MDS principal coordinates
dayrng = c(0,365)
nbasis <- 15 # Try later with 65
fbasis = create.fourier.basis(dayrng,nbasis)
tempfd <- with(CanadianWeather,
smooth.basis(day.5, dailyAv[,,1],
fbasis,
fdnames=list("Day", "Station", "Deg C"))$fd )
tempPCA = pca.fd(tempfd,nharm=6)
scores = tempPCA$scores[,1:3]
plot(data.frame(scores,mds.scores))
mds.sm <- cmdscale(d.temp.sm,k=30,eig=TRUE)
mds.sm.scores = mds.sm$points[,1:3]
plot(as.data.frame(mds.sm.scores))
# % of explained variance by each principal component
round(mds.sm$eig/sum(mds.sm$eig)*100,2)
# cumulated
round(cumsum(mds.sm$eig)/sum(mds.sm$eig)*100,2)
plot(mds.sm$eig/sum(mds.sm$eig)*100,type="b",
main="Percentage of variance explained")
# MDS scores maps
plot(mds.sm.scores[,1],mds.sm.scores[,2],xlab="PrCo 1",ylab="PrCo 2")
text(mds.sm.scores[,1],mds.sm.scores[,2],CanadianWeather$place,pos=3)
plot(-mds.sm.scores[,3],mds.sm.scores[,1],xlab="-PrCo 3",ylab="PrCo 1")
text(-mds.sm.scores[,3],mds.sm.scores[,1],CanadianWeather$place,pos=3)
plot(data.frame(mds.scores,mds.sm.scores))
#
# Clustering
#
hcl.m <- hclust(as.dist(d.temp.m), method="single")
plot(hcl.m,labels=CanadianWeather$place)
hcl.sm <- hclust(as.dist(d.temp.sm), method="single")
plot(hcl.sm,labels=CanadianWeather$place)
library(tidyverse)
library(fda)
library(ggpubr)
install.packages('ggpubr')
install.packages("ggpubr")
install.packages("ggpubr")
install.packages("ggpubr")
library(tidyverse)
# --- Environment setup ---
setwd('/Users/efwerr/GitHub/AML/Lab6')
set.seed(6046)
library(kernlab) # kernlab is a very powerful R library for kernel methods
reuters <- read.table("reuters.txt.gz", header=T)
# We leave only three topics for analysis: Crude Oil, Coffee and Grain-related news
reuters <- reuters[reuters$Topic == "crude" | reuters$Topic == "grain" | reuters$Topic == "coffee",]
reuters$Content <- as.character(reuters$Content)    # R originally loads this as factor, so needs fixing
reuters$Topic <- factor(reuters$Topic)              # re-level the factor to have only three levels
levels(reuters$Topic)
length(reuters$Topic)
table(reuters$Topic)
## an example of a text about coffee
reuters[2,]
## an example of a text about grain
reuters[7,]
## an example of a text about crude oil
reuters[12,]
(N <- dim(reuters)[1])  # number of rows
reuters <- reuters[sample(1:N, N),]
# we can define a normalized 3-spectrum kernel (n is length)
k <- stringdot("spectrum", length=3, normalized=TRUE)
Frank.Sinatra <- "I did it my way"
k(Frank.Sinatra, Frank.Sinatra)
k(Frank.Sinatra, "He did it his way")
k(Frank.Sinatra, "She did it her way")
k(Frank.Sinatra, "Let's find our way out")
k(Frank.Sinatra, "Brexit means Brexit")
plotting <-function (kernelfu, kerneln)
{
xpercent <- eig(kernelfu)[1]/sum(eig(kernelfu))*100
ypercent <- eig(kernelfu)[2]/sum(eig(kernelfu))*100
plot(rotated(kernelfu), col=as.integer(reuters$Topic),
main=paste(paste("Kernel PCA (", kerneln, ")", format(xpercent+ypercent,digits=3)), "%"),
xlab=paste("1st PC -", format(xpercent,digits=3), "%"),
ylab=paste("2nd PC -", format(ypercent,digits=3), "%"))
}
k <- stringdot("spectrum", length=5, normalized=TRUE)
K <- kernelMatrix(k, reuters$Content)
dim(K)
K[2,2]
K[2,3:10]
kpc.reuters <- kpca (K, features=2, kernel="matrix")
plotting (kpc.reuters,"5 - spectrum kernel")
## finally add a legend
legend("bottomleft", legend=c("crude oil", "coffee","grain"),
pch=c(1,1),                    # gives appropriate symbols
col=c("red","black", "green"), # gives the correct color
bg="transparent",              # makes the legend transparent
cex = 0.7)                     # legend size
## First we should split the data into learning (2/3) and test (1/3) parts
ntrain <- round(N*2/3)     # number of training examples
tindex <- sample(N,ntrain) # indices of training examples
## The fit a SVM in the train part
svm1.train <- ksvm (K[tindex,tindex],reuters$Topic[tindex], type="C-svc", kernel='matrix')
# First the test-vs-train matrix
testK <- K[-tindex,tindex]
# then we extract the SV from the train
testK <- testK[,SVindex(svm1.train),drop=FALSE]
# Now we can predict the test data
# Warning: here we MUST convert the matrix testK to a 'kernelMatrix'
y1 <- predict(svm1.train,as.kernelMatrix(testK))
table (pred=y1, truth=reuters$Topic[-tindex])
K[2,2]
K[2,3:10]
kpc.reuters <- kpca (K, features=2, kernel="matrix")
plotting (kpc.reuters,"5 - spectrum kernel")
## finally add a legend
legend("bottomleft", legend=c("crude oil", "coffee","grain"),
pch=c(1,1),                    # gives appropriate symbols
col=c("red","black", "green"), # gives the correct color
bg="transparent",              # makes the legend transparent
cex = 0.7)                     # legend size
## First we should split the data into learning (2/3) and test (1/3) parts
ntrain <- round(N*2/3)     # number of training examples
tindex <- sample(N,ntrain) # indices of training examples
## The fit a SVM in the train part
svm1.train <- ksvm (K[tindex,tindex],reuters$Topic[tindex], type="C-svc", kernel='matrix')
# First the test-vs-train matrix
testK <- K[-tindex,tindex]
# then we extract the SV from the train
testK <- testK[,SVindex(svm1.train),drop=FALSE]
# Now we can predict the test data
# Warning: here we MUST convert the matrix testK to a 'kernelMatrix'
y1 <- predict(svm1.train,as.kernelMatrix(testK))
table (pred=y1, truth=reuters$Topic[-tindex])
cat('Error rate = ',100*sum(y1!=reuters$Topic[-tindex])/length(y1),'%')
library("rgl")
open3d()
plotting3D <-function (kernelfu, kerneln)
{
xpercent <- eig(kernelfu)[1]/sum(eig(kernelfu))*100
ypercent <- eig(kernelfu)[2]/sum(eig(kernelfu))*100
zpercent <- eig(kernelfu)[3]/sum(eig(kernelfu))*100
# resize window
par3d(windowRect = c(100, 100, 612, 612))
plot3d(rotated(kernelfu),
col  = as.integer(reuters$Topic),
xlab = paste("1st PC -", format(xpercent,digits=3), "%"),
ylab = paste("2nd PC -", format(ypercent,digits=3), "%"),
zlab = paste("3rd PC -", format(zpercent,digits=3), "%"),
main = paste("Kernel PCA"),
sub = "red - crude oil | black - coffee | green - grain",
top = TRUE, aspect = FALSE, expand = 1.03)
}
kpc.reuters <- kpca (K, features=3, kernel="matrix")
plotting3D (kpc.reuters,"5 - spectrum kernel")
set.seed(6046)
library(kernlab) # kernlab is a very powerful R library for kernel methods
reuters <- read.table("reuters.txt.gz", header=T)
# We leave only three topics for analysis: Crude Oil, Coffee and Grain-related news
reuters <- reuters[reuters$Topic == "crude" | reuters$Topic == "grain" | reuters$Topic == "coffee",]
reuters$Content <- as.character(reuters$Content)    # R originally loads this as factor, so needs fixing
reuters$Topic <- factor(reuters$Topic)              # re-level the factor to have only three levels
levels(reuters$Topic)
length(reuters$Topic)
table(reuters$Topic)
## an example of a text about coffee
reuters[2,]
## an example of a text about grain
reuters[7,]
## an example of a text about crude oil
reuters[12,]
(N <- dim(reuters)[1])  # number of rows
reuters <- reuters[sample(1:N, N),]
# we can define a normalized 3-spectrum kernel (n is length)
k <- stringdot("spectrum", length=3, normalized=TRUE)
Frank.Sinatra <- "I did it my way"
k(Frank.Sinatra, Frank.Sinatra)
k(Frank.Sinatra, "He did it his way")
k(Frank.Sinatra, "She did it her way")
k(Frank.Sinatra, "Let's find our way out")
k(Frank.Sinatra, "Brexit means Brexit")
plotting <-function (kernelfu, kerneln)
{
xpercent <- eig(kernelfu)[1]/sum(eig(kernelfu))*100
ypercent <- eig(kernelfu)[2]/sum(eig(kernelfu))*100
plot(rotated(kernelfu), col=as.integer(reuters$Topic),
main=paste(paste("Kernel PCA (", kerneln, ")", format(xpercent+ypercent,digits=3)), "%"),
xlab=paste("1st PC -", format(xpercent,digits=3), "%"),
ylab=paste("2nd PC -", format(ypercent,digits=3), "%"))
}
k <- stringdot("spectrum", length=5, normalized=TRUE)
K <- kernelMatrix(k, reuters$Content)
dim(K)
K[2,2]
K[2,3:10]
kpc.reuters <- kpca (K, features=2, kernel="matrix")
plotting (kpc.reuters,"5 - spectrum kernel")
## finally add a legend
legend("bottomleft", legend=c("crude oil", "coffee","grain"),
pch=c(1,1),                    # gives appropriate symbols
col=c("red","black", "green"), # gives the correct color
bg="transparent",              # makes the legend transparent
cex = 0.7)                     # legend size
## First we should split the data into learning (2/3) and test (1/3) parts
ntrain <- round(N*2/3)     # number of training examples
tindex <- sample(N,ntrain) # indices of training examples
## The fit a SVM in the train part
svm1.train <- ksvm (K[tindex,tindex],reuters$Topic[tindex], type="C-svc", kernel='matrix')
# First the test-vs-train matrix
testK <- K[-tindex,tindex]
# then we extract the SV from the train
testK <- testK[,SVindex(svm1.train),drop=FALSE]
# Now we can predict the test data
# Warning: here we MUST convert the matrix testK to a 'kernelMatrix'
y1 <- predict(svm1.train,as.kernelMatrix(testK))
table (pred=y1, truth=reuters$Topic[-tindex])
cat('Error rate = ',100*sum(y1!=reuters$Topic[-tindex])/length(y1),'%')
library("rgl")
open3d()
plotting3D <-function (kernelfu, kerneln)
{
xpercent <- eig(kernelfu)[1]/sum(eig(kernelfu))*100
ypercent <- eig(kernelfu)[2]/sum(eig(kernelfu))*100
zpercent <- eig(kernelfu)[3]/sum(eig(kernelfu))*100
# resize window
par3d(windowRect = c(100, 100, 612, 612))
plot3d(rotated(kernelfu),
col  = as.integer(reuters$Topic),
xlab = paste("1st PC -", format(xpercent,digits=3), "%"),
ylab = paste("2nd PC -", format(ypercent,digits=3), "%"),
zlab = paste("3rd PC -", format(zpercent,digits=3), "%"),
main = paste("Kernel PCA"),
sub = "red - crude oil | black - coffee | green - grain",
top = TRUE, aspect = FALSE, expand = 1.03)
}
kpc.reuters <- kpca (K, features=3, kernel="matrix")
plotting3D (kpc.reuters,"5 - spectrum kernel")
library(igraph)
library(diffuStats) # if in trouble, go to https://www.bioconductor.org/packages/release/bioc/html/diffuStats.html
install.packages('igraph')
install.packages('diffuStats')
install.packages('expm')
library(igraph)
library(diffuStats) # if in trouble, go to https://www.bioconductor.org/packages/release/bioc/html/diffuStats.html
library(expm)
options (digits=3)
(g <- graph_from_literal (Fred-Bobby-Georgina-Fred, Fred-Charlie, Bobby-David-Anne-Georgina))
# Creation of a toy similarity measure
S <- diag(6)
names <- c("Fred","Bobby","Georgina","Charlie","David","Anne")
dimnames (S) <- list(names,names)
# Strength of relationships
S["Fred","Bobby"] <- 0.1
S["Fred","Georgina"] <- 0.3
S["Fred","Charlie"] <- 0.2
S["Bobby","Georgina"] <- 0.9
S["Bobby","David"] <- 0.4
S["Georgina","Anne"] <- 0.7
S["David","Anne"] <- 0.5
(S[lower.tri(S)] = t(S)[lower.tri(S)])
# add the node-node similarities to the graph
g <- graph_from_adjacency_matrix(S *
as.matrix(get.adjacency(g, type="both")), mode="undirected", weighted=TRUE)
# plot it
plot(g, edge.label=E(g)$weight)
eigen(S, only.values = TRUE)$values # S need not be psd
lambda <- 0.5
(K <- expm(lambda*S))
eigen(K, only.values = TRUE)$values # K is pd
normalize.kernel = function(K)
{
k = 1/sqrt(diag(K))
K * (k %*% t(k))
}
(K.n <- normalize.kernel(K))
# this step did not affect the relationships substantially
cor (rowSums(S), rowSums(K.n))
g <- generate_graph(fun_gen = igraph::barabasi.game,
param_gen = list(n = 100, m = 3, directed = FALSE),
seed = 1)
com <- cluster_spinglass(g, spins=3)
V(g)$color <- com$membership
g <- set_graph_attr(g, "layout", layout_with_kk(g))
plot(g, vertex.label.dist=1)
(S[lower.tri(S)] = t(S)[lower.tri(S)])
# add the node-node similarities to the graph
g <- graph_from_adjacency_matrix(S *
as.matrix(get.adjacency(g, type="both")), mode="undirected", weighted=TRUE)
# plot it
plot(g, edge.label=E(g)$weight)
eigen(S, only.values = TRUE)$values # S need not be psd
# plot it
plot(g, edge.label=E(g)$weight)
eigen(S, only.values = TRUE)$values # S need not be psd
lambda <- 0.5
(K <- expm(lambda*S))
eigen(K, only.values = TRUE)$values # K is pd
normalize.kernel = function(K)
{
k = 1/sqrt(diag(K))
K * (k %*% t(k))
}
(K.n <- normalize.kernel(K))
# this step did not affect the relationships substantially
cor (rowSums(S), rowSums(K.n))
g <- generate_graph(fun_gen = igraph::barabasi.game,
param_gen = list(n = 100, m = 3, directed = FALSE),
seed = 1)
com <- cluster_spinglass(g, spins=3)
V(g)$color <- com$membership
g <- set_graph_attr(g, "layout", layout_with_kk(g))
plot(g, vertex.label.dist=1)
colorets <- c("skyblue2","orange3","green4")
legend("bottomleft",
c('Statistics','CompSci','Math'),
col=colorets,pch=1,text.col=colorets,
bg = "transparent", cex = 0.7)
K.diff <- diffusionKernel(g, 1)
is_kernel(K.diff)
hist(eigen(K.diff, only.values = TRUE)$values, breaks = 30, main = "Eigenvalue histogram")
K.diff[1:7,1:7]
K.norm <- normalize.kernel(K.diff)
is_kernel(K.norm)
K.norm[1:7,1:7]
K.svm <- as.kernelMatrix(K.norm)
t <- factor(V(g)$color)
# save(K.norm,t, file="graph-kernels")
load(file="graph-kernels")
K.svm <- as.kernelMatrix(K.norm)
